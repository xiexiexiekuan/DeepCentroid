import time
import pandas
import centroid_compute
import numpy
from sklearn import metrics
import partition
import simple_centroid
from centroid_compute import threshold
from sklearn.preprocessing import scale, normalize, minmax_scale
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn import svm

##########################################################################################
# This file corresponds to ablation experiment 1, Random forest Integration testing, and is used by replacing the centroid file of main.py file with centroid_ RF is sufficient

centroid_vector = []  # The centroid vector generated by the first layer centroid classifier
gene_partition_list = []  # Centroid classifier gene set
extra_partition_list = []  # Additional data gene set
level_data = []  # Transfer data between layers
layer = 1  # Number of training layers
valid_layer = 1  # Effective training layers
extra_vector = []
probability = []
feature_select = []
is_feature_select = False  # Feature filtering or not
tree_model = []

# initialization
def initialization(select, gene_set_num, gene_set_min, gene_set_max):
    partition.initialization(gene_set_num, gene_set_min, gene_set_max)
    centroid_compute.initialization(gene_set_num)
    global centroid_vector, gene_partition_list, extra_partition_list, level_data, layer, valid_layer, extra_vector, feature_select, is_feature_select, tree_model
    centroid_vector = []
    gene_partition_list = []
    extra_partition_list = []
    level_data = []
    layer = 1
    valid_layer = 1
    extra_vector = []
    feature_select = []
    is_feature_select = select
    tree_model = []


# train
def train(train_data_, train_label, max_train_layer, known_feature_set, bootstrap_size, is_sliding_scan, cut_centroid, function_annotation):
    global layer, valid_layer, gene_partition_list, extra_partition_list, level_data, probability, extra_vector, feature_select, tree_model

    if is_feature_select:
        data = SelectFromModel(RandomForestClassifier()).fit(train_data_, train_label)
        feature_select = data.get_support()
        # print(train_data_.shape)
        train_data_ = train_data_[:, feature_select]
        print('feature_select: ', train_data_.shape)

    # Obtain the training set, training label, validation set, and validation label corresponding to each gene set
    probability = centroid_compute.probability(train_label)
    mcc = 0
    express_data = train_data_

    # Cascading training
    while True:
        if max_train_layer < layer:
            print('The training level is too high, the training is over!')
            break

        print('{} layer training:'.format(layer), end=" ")
        layer_time = time.time()  # Calculation time of this layer

        if layer >= 2:
            if not is_sliding_scan:
                extra_list = partition.random_cut_data(train_data_.shape[1])
            else:
                extra_list = partition.random_cut_data_order(train_data_.shape[1])
            extra_partition_list.append(extra_list)
            statistics_data = centroid_compute.gene_set_statistics(extra_list, train_data_)
            #Splice the previous layer of output data and new data by column
            express_data = numpy.append(level_data, statistics_data, axis=1).astype(numpy.float32)

        print("Input data:", express_data.shape)

        # Number of gene sets
        if not is_sliding_scan:
            partition_list = partition.random_cut_data(express_data.shape[1])  # Divide a random gene set
        else:
            partition_list = partition.random_cut_data_order(express_data.shape[1])  # Divide a random gene set
        # If adding a set of known genes, add them in the first layer
        if layer == 1 and known_feature_set:
            partition_list = partition.get_known_set(partition_list)  # Obtain a set of known genes
        partition_num = partition_list.shape[0]

        data_cut_set = centroid_compute.data_divide(partition_num, bootstrap_size, partition_list, express_data, train_label, probability)
        gene_partition_list.append(partition_list)

        # No pruning and gene annotation functions provided
        model = []
        tree_level_data = []
        for index, item in enumerate(data_cut_set):
            m = RandomForestClassifier()
            m.fit(item[0], item[1])
            p = m.predict_proba(express_data[:, partition_list[index]])
            p = numpy.array(p[:, 1]).T
            tree_level_data.append(p)
            model.append(m)
        tree_model.append(model)

        y_pred_p = numpy.average(tree_level_data, axis=0)
        y_pred = numpy.int64(y_pred_p >= 0.5)

        accuracy = metrics.accuracy_score(train_label, y_pred)
        auc = metrics.roc_auc_score(train_label, y_pred_p)
        mcc_score_ = metrics.matthews_corrcoef(train_label, y_pred)
        print("Training Set Prediction Results ACC = {:.5f}, AUC = {:.5f}, MCC = {:.5f}".format(accuracy, auc, mcc_score_), end="  ")
        level_data = numpy.array(tree_level_data).T

        print("timeï¼š{:.2f} min.".format((time.time() - layer_time) / 60.0))
        # Control the number of layers in the model and stop running when the MCC grows too little
        valid_layer = layer

        if mcc_score_ <= mcc or layer == 1: # The integrated Random forest fitting is too strong, and the test set mcc=0 in the second layer, so only one layer is set
            # valid_layer -= 1
            break
        else:
            mcc = mcc_score_
        layer += 1  # Add one layer to the number of layers


def predict(test_data, test_data_label):
    result_p = []
    result_set = []

    if is_feature_select:
        test_data = test_data[:, feature_select]
    c_distance = test_data

    for i in range(0, valid_layer):
        result = []
        if i > 0:
            statistics_test = centroid_compute.gene_set_statistics(extra_partition_list[i - 1], test_data)
            c_distance = numpy.append(c_distance, statistics_test, axis=1)

        partition_list = gene_partition_list[i]
        for index, model in enumerate(tree_model[i]):
            result_ = model.predict_proba(c_distance[:, partition_list[index]])
            result_ = numpy.array(result_[:, 1]).T
            result.append(result_)

        c_distance = numpy.array(result).T
        result_p = numpy.mean(result, axis=0)
        result = numpy.int64(result_p >= 0.5)
        acc_score = metrics.accuracy_score(test_data_label, result)
        precision = metrics.precision_score(test_data_label, result)
        recall = metrics.recall_score(test_data_label, result)
        f1_score = metrics.f1_score(test_data_label, result)
        auc_score = metrics.roc_auc_score(test_data_label, result_p)
        mcc_score = metrics.matthews_corrcoef(test_data_label, result)
        print("{} layer test set prediction results ACC = {:.5f}, AUC = {:.5f}, MCC = {:.5f}".format(i + 1, acc_score, auc_score, mcc_score))

        result_set = numpy.array([acc_score, precision, recall, f1_score, auc_score, mcc_score])

    return result_set, test_data_label, result_p
